import itertools, random, warnings
import crossval

import numpy as np

from pylearn2.monitor import Monitor
from pylearn2.training_algorithms.sgd import ExhaustiveSGD
from pylearn2.costs.autoencoder import MeanSquaredReconstructionError
from pylearn2.training_algorithms.sgd import EpochCounter
from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix
import pylearn2.scripts.deep_trainer.run_deep_trainer as dp

"""
    The C's enum-like python class for specifying the data type of a certain
    hyperparameter.
"""
class DataConversionFct:
    INT = 'to_int'
    FLOAT = 'to_float'

"""
    The C's enum-like python class for specifying if the values associated with
    a certain hyperparameter correspond to a 1) range of values upon which random
    values should be generated or 2) list of values the search algorithm should
    use when generating the different combinations of hyperparameters values.
"""
class TypeHyperparam:
    LIST = 'list'
    RANGE = 'range'

class HyperparamOptimization(object):
    """
    A class that performs hyperparameter optimization according to grid search
    or random search along with cross-validation.
    """
    _default_seed = (17, 2, 946)

    def __init__(self, model, dataset, algorithm, var_params, fixed_params,
                 cost, validation_batch_size, train_prop, rng=_default_seed):
        """
        Construct a ParamOptimization instance.

        Parameters
        ----------
        model : object
            Object that implements the Model interface defined in
            `pylearn2.models`.
        dataset : object
            Object that implements the Dataset interface defined in
            `pylearn2.datasets`.
        algorithm: class, optional
            Class that implements the TrainingAlgorithm interface defined in
            `pylearn2.training_algorithms`.
        var_params: list
            List of objects of type `VarHyperparam`. Each of this object
            corresponds to a hyperparameter which we associate a list of values
            to be used with the training algorithm.
        fixed_params: dict
            Dict of fixed hyperparameters. The key is the name of the
            hyperparameter and it is associated with the fixed hyperparamter
            value.
        cost : object, optional
            Object that basically evaluates the model and computes the cost.
        validation_batch_size: int, optional
            Batch size per update. TODO: What if this is not provided?
        train_prop: int
            The proportion of the training set with respect to the
            validation set.
        rng : object, optional
            Random number generation class to be used.
        """
        self.model = model
        self.dataset = dataset
        self.algorithm = algorithm
        self.var_params = var_params
        # Check that all the hyperparameters are unique.
        self.check_params(var_params)
        self.fixed_params = fixed_params
        self.cost = cost
        self.validation_batch_size = validation_batch_size
        self.train_prop = train_prop
        self.rng = np.random.RandomState(rng)

    def check_params(self, params):
        """
        Train the algorithm with a set of parameters values generated by the
        grid or search algorithm.

        Parameters
        ----------
        var_params: list
            List of objects of type `VarHyperparam`. Each of this object
            corresponds to a hyperparameter which we associate a list of values
            to be used with the training algorithm.
        """
        param_names = map(lambda x: x.name, params)
        param_count = {}
        seen = set()
        seen_add = seen.add
        param_duplicates = list(set(param for param in param_names
                                    if param in seen or seen_add(param)))
        ParamDuplicatesWarning(param_duplicates)

    def __call__(self, crossval_mode, search_mode='random_search',
                 n_combinations=5):
        """
        It executes the selected search algorithm by iterating the training
        algorithm throughout the generated set of parameters combinations.

        Parameters
        ----------
        crossval_mode : string
            Cross validation mode to be applied on the dataset.
            `kfold` and `holdout` are the supported cross-validation modes.
        search_mode : string
            The type of parameter search algorithm (random_search or
            grid_seach) to be used for hyperparameter optimization.
        n_combinations: int, optional
            This option is associated with the random_search algorithm and it
            specifies the number of combinations of parameters values to
            test for.
        """
        self.crossval_mode = crossval_mode
        self.n_combinations = n_combinations
        if search_mode == 'random_search':
            self.random_search()
        elif search_mode == 'grid_search':
            self.grid_search()
        else:
            raise ValueError('The search mode specified is not supported.')

    def random_search(self):
        """
        Implementation of the (jumping) random search algorithm.
        It selects `n_combinations` parameter combinations and calls a Train
        function for each set of parameters values. For each parameter of type
        `range` (see `VarHyperparams` below), the values are generated randomly
        from a uniform distribution.
        """
        # Generate randomly the values for each parameter of type `range`.
        map(lambda x: x.generate_random_values(self.n_combinations, self.rng),
            self.var_params)
        # Generate `n_combinations` combinations of parameters values.
        combinations = self.generate_params_combinations()
        funct_iterator = itertools.imap(self.train_algo, combinations)
        self.funct_rval = list(funct_iterator)

    def generate_params_combinations(self):
        """
        Generates combinations of parameters values when executing the random
        search algorithm. The number of combinations generated is specified
        by `n_combinations`.
        """
        index_param = 0
        temp = []
        combinations = [[] for i in xrange(self.n_combinations)]
        for param in self.var_params:
            for index_param in xrange(self.n_combinations):
                if param.type_param == 'list':
                    combinations[index_param].append(self.rng.permutation(param.values)[0])
                else:
                    combinations[index_param].append(param.discretized_interval[0][index_param])
        return combinations

    def grid_search(self):
        """
        Implementation of the grid search algorithm.
        It generates all the possible parameter combinations and calls a Train
        function for each set of parameters values.
        """
        # Discretize the hyperparameter space.
        import pdb; pdb.set_trace()
        params_values = map(lambda x: x.discretize(), self.var_params)
        funct_iterator = itertools.imap(self.train_algo,
                [config for config in itertools.product(*params_values)])
        self.funct_rval = list(funct_iterator)
        import pdb; pdb.set_trace()

    def train_algo(self, var_params_val):
        """
        Train the algorithm with a set of parameters values generated by the
        grid or search algorithm.

        Parameters
        ----------
        var_params_val: list
            The list of hyperparameters values to be tested with the
            training algorithm.
        """
        # Check the monitor's channels.
        import pdb; pdb.set_trace()
        if hasattr(self.model, 'monitor'):
            del self.model.monitor
        config = {}
        var_params_config = dict(zip(map(lambda x: x.name, self.var_params),
                                var_params_val))
        config.update(self.fixed_params)
        config.update(var_params_config)
        train_algo = self.algorithm(**config)
        if self.crossval_mode == 'kfold':
            #crossval.KFoldCrossValidation
            kfoldCV = crossval.KFoldCrossValidation(model=self.model,
                            algorithm=train_algo,
                            cost=MeanSquaredReconstructionError(),
                            dataset=self.dataset,
                            validation_batch_size=self.validation_batch_size)
            kfoldCV.crossvalidate_model()
            print "Error: " + str(kfoldCV.get_error())
            return [kfoldCV.get_error(), var_params_config]
        elif self.crossval_mode == 'holdout':
            holdoutCV = crossval.HoldoutCrossValidation(model=self.model,
                            algorithm=train_algo,
                            cost=MeanSquaredReconstructionError(),
                            dataset=self.dataset,
                            validation_batch_size=self.validation_batch_size,
                            train_prop=self.train_prop)
            holdoutCV.crossvalidate_model()
            print "Error: " + str(holdoutCV.get_error())
            return [holdoutCV.get_error(), var_params_config]
        else:
            raise ValueError("The specified crossvalidation mode %s is not supported"
                             %self.crossval_mode)

    def get_top_N_exp(self, n_exps):
        """
        Gets the best N experiments based on the cross-validation error.

        Parameters
        ----------
        n_exps: int
            The number of best experiments to collect based on the
            cross-validation error.
        """
        best_exp = sorted(self.funct_rval)[0]
        import pdb; pdb.set_trace()
        return {'error':best_exp[0],  'var_params':best_exp[1]}

get_conv_fct = {'to_int': round, 'to_float': float}

class VarHyperparam(object):
    def __init__(self, name, type_param, values, n_grid_points=10, type_data='to_float'):
        """
        Constructor that is used for storing the hyperparameters values used on
        a training algorithm when performing cross-validation.

        Parameters
        ----------
        name : string
            The name of the hyperparameter.
        type_param : string
            Can take one of the 2 values `list` or `range`.
            It specifies if the values associated with the parameter are a list
            of values to be tested or a range of values upon which the random
            or grid search algorithms must generate values.
        values: list
            A list of values that the parameter can take if the parameter is of
            `list` type.  If it is of `range` type, then the list corresponds
            to the range of values upon which the search algorithms will
            generate values.
        n_grid_points: int, optional
            This option is associated with the grid search algorithm and it
            specifies the number of grid points to generate along the
            hyperparameter axis.
        TODO:
        type_data: python data type conversion built-in function.
            Can be any of the python built-in function for converting from one
            data type to another such as float() or int().
        """
        self.name = name
        self.type_param = type_param
        # TODO: sort values.
        self.values = sorted(values)
        # Check if the parameter is specified correctly.
        self.check_param_spec()
        self.n_grid_points = n_grid_points
        self.type_data = type_data
        self.data_conversion_fct = get_conv_fct[self.type_data]
        self.discretized_interval = None

    def check_param_spec(self):
        """
        Checks that the information provided for the variable hyperparameter is
        valid.
        """
        error_msg = 'The parameter %s is not specified correctly.' %self.name
        if self.type_param == 'range' and len(self.values) != 2:
            raise RangeParamError1(error_msg +
                 'For the range type of parameter, you must specify exactly'
                 ' both a lower bound value and upper bound value.')
        elif self.type_param == 'range' and 'bool' in str(type(self.values[0])):
            raise RangeParamError2(error_msg +
                  ' For the range type of parameter, you can not specify a'
                  ' boolean lower or upper bound.')
        elif self.type_param == 'list' and len(self.values) == 0:
            raise ListParamError1(error_msg +
                  ' For the list type of parameter, you must specify at least'
                  ' a value for the parameter to take.')

    def discretize(self):
        """
        For a given hyperparameter of `range` type, it discretizes the
        hyperparameter axis. The number of grid points is specified by
        `n_grid_points`.
        """
        import pdb; pdb.set_trace()
        if self.type_param == 'range':
            if (type(self.values[1]) is float or type(self.values[0]) is float) and self.type_data == 'to_int':
                self.type_data = 'to_float'
                self.data_conversion_fct = get_conv_fct[self.type_data]
                RangeParamWarning1(self.name)
            interval = self.data_conversion_fct(float(self.values[1] - self.values[0]) / self.n_grid_points)
            if interval == 0:
                interval = 1
            n_points = self.n_grid_points
            if self.type_data == 'to_int':
                n_points = self.n_grid_points - 1
            self.discretized_interval = [self.data_conversion_fct(self.values[0] + value*interval)
                for value in xrange(n_points)
                if self.data_conversion_fct(self.values[0] + value*interval)  < self.values[1]]
            '''
            if self.type_data == 'to_int':
                if self.values[1] - self.discretized_interval[-1] < interval:
                    self.discretized_interval.pop()
            '''
            self.discretized_interval.append(self.values[1])
            import pdb; pdb.set_trace()
            if len(self.discretized_interval) != self.n_grid_points:
                RangeParamWarning2(self.name, self.n_grid_points,
                                     self.discretized_interval, self.values)
            return self.discretized_interval
        else:
            return self.values

    def generate_random_values(self, n_random_values, rng):
        """
        For a given hyperparameter of `range` type, it generates random
        values from a uniform distribution. The number of random values
        is specified by `n_random_values`.

        Parameters
        ----------
        n_random_values : int
            Number of random values to generate for the given hyperparameter.
        rng : object
            Random number generation class to be used.
        """
        if self.type_param == 'range':
            if self.type_data == 'to_float':
                self.discretized_interval = rng.uniform(self.values[0],
                                                        self.values[1],
                                                        (1, n_random_values))
            elif self.type_data == 'to_int':
                self.discretized_interval = rng.random_integers(self.values[0],
                                                                self.values[1],
                                                           (1, n_random_values))


# Definition of exceptions and warnings in this module.
class RangeParamError1(Exception):
    """
       Exception raised when not specifying a lower or upper bounds for the
       range type of parameter.
    """
    pass
class RangeParamError2(Exception):
    """
       Exception raised when giving a boolean lower or upper bound for the
       range type of parameter.
    """
    pass
class ListParamError1(Exception):
    """
       Exception raised when not giving at least a value for the list type
       of parameter.
    """
    pass

class RangeParamWarning1(Warning):
    """
       Warning issued when giving a lower and upper bound for the ranges
       type of parameter less than 1 and the hyperameter data type is integer.
    """
    def __init__(self, name):
        warnings.warn('Your lower and/or uppper bound values'
        ' is not an integer and you specified INT as the data type'
        ' for the hyperparameter %s.'%name +
        '\nThus, the data type for the hyperameter %s has been set'
        ' to FLOAT.' %name)

class RangeParamWarning2(Warning):
    """
       Warning issued when it was not possible to generate the user-defined
       number of grid points for a range type of hyperparameter. It was not
       possible to satisfy the user's specification on the hyperparameter's
       data type and range of values.
    """
    def __init__(self, name, n_grid_points, discretized_interval, values):
        warnings.warn('The user-specified number of grid points for'
            ' the hyperparameter %s is %s. But only %s grid points were'
            ' generated in the interval [%s, %s].'
            %(name, n_grid_points, len(discretized_interval),
              values[0], values[1]))

class ParamDuplicatesWarning(Warning):
    """
       Warning issued when more than range of values was given for a
       hyperameter.
    """
    def __init__(self, param_duplicates):
        warnings.warn('More than one range of values was defined for the'
            ' following hyperparameters: %s.'
            '\n The last range of values defined for a duplicated hyperparameter will'
            ' take precedence over the other range of values.'
            %(param_duplicates))


# Testing the cross-validation implementation by generating the parameters
# ranges automatically and manually giving the values to be tested for some
# parameters.
def test():
    trainset, testset = dp.get_dataset_toy()
    design_matrix = trainset.get_design_matrix()
    n_input = design_matrix.shape[1]
    structure = [n_input, 400]
    model = dp.get_denoising_autoencoder(structure)
    var_params = [
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [1, 5], 3, DataConversionFct.INT),
                  #VarHyperparam('learning_rate', TypeHyperparam.RANGE, [8, 15.21], 10, DataConversionFct.FLOAT),
                  #VarHyperparam('learning_rate', TypeHyperparam.RANGE, [0.2, 5.21], 5, DataConversionFct.INT),
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [0.2, 5], 10, DataConversionFct.INT),
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [0.2, 5], 10, DataConversionFct.FLOAT),
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [1, 5.5], 10, DataConversionFct.FLOAT),
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [1, 5.5], 10, DataConversionFct.INT),
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [-1, 5.5], 10, DataConversionFct.INT),
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [-1, -15.5], 10, DataConversionFct.INT),
                  VarHyperparam('learning_rate', TypeHyperparam.RANGE, [-1, -15.5], 10, DataConversionFct.FLOAT),
                  VarHyperparam('batch_size', TypeHyperparam.RANGE, [10, 15], 9, DataConversionFct.INT),
                  VarHyperparam('monitoring_batches', TypeHyperparam.RANGE, [16, 20], 4, DataConversionFct.FLOAT),
                  VarHyperparam('termination_criterion', TypeHyperparam.LIST,
                                [EpochCounter(max_epochs=10),
                                EpochCounter(max_epochs=20)]),
                 ]
    fixed_params = {'cost': MeanSquaredReconstructionError(),
                    'update_callbacks': None
                   }
    param_optimizer = HyperparamOptimization(model=model, dataset=trainset,
                            algorithm=ExhaustiveSGD, var_params=var_params,
                            fixed_params=fixed_params,
                            cost=MeanSquaredReconstructionError(),
                            validation_batch_size=1000, train_prop=0.5,
                            )
    '''
    param_optimizer(crossval_mode='holdout',
                    search_mode='random_search',
                    n_combinations=10)
    '''

    param_optimizer(crossval_mode='holdout',
                    search_mode='grid_search')


if __name__ == '__main__':
    test()