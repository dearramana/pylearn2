The files in this directory recreate the experiment reported in the
paper

An efficient learning procedure for Deep Boltzmann Machines. G. Hinton, and R. Salakhutdinov.

The procedure is divided in three phases: pretraining of RBMs, training and finetuning. The test_dbm_mnist script allows to enable each phase of training and to select whether the DBM
is composed of a softmax layer or not, and whether the MLP has to use dropout or not.

As explained in the paper, the finetuning procedure uses an augmented input to feed the MLP and this implementation creates it using augment_input.py and
mnistaugmented.py in pylearn2/datasets/. The latter uses .pkl files of the training set and test set and augment them. In order to make this procedure easier, I used .csv files of the
mnist training set and test set made by me. Simply change the first_path variable in mnistaugmented to use them.

NO DROPOUT RESULTS:
The procedure without dropout does not need to use a softmax layer for the DBM and using all the provided parameters it should give you an test_y_misclass of 0.0094%.
DROPOUT RESULTS:
The procedure with dropout uses a softmax layer for the DBM and using all the provided parameters it should give you an test_y_misclass of 0.0084%.